[[34m2024-06-25T11:21:39.528+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2024-06-25T11:21:39.538+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-06-25T11:21:39.543+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 6775[0m
[[34m2024-06-25T11:21:39.544+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T11:21:39.547+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-06-25T11:21:39.716+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-06-25T11:21:39.768+0000[0m] {[34mscheduler_job_runner.py:[0m1576} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-06-25T11:22:16.486+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: transform_dag.transform_task manual__2024-06-25T11:22:15.677668+00:00 [scheduled]>[0m
[[34m2024-06-25T11:22:16.487+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG transform_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T11:22:16.487+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: transform_dag.transform_task manual__2024-06-25T11:22:15.677668+00:00 [scheduled]>[0m
[[34m2024-06-25T11:22:16.489+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='transform_dag', task_id='transform_task', run_id='manual__2024-06-25T11:22:15.677668+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-25T11:22:16.489+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'transform_dag', 'transform_task', 'manual__2024-06-25T11:22:15.677668+00:00', '--local', '--subdir', 'DAGS_FOLDER/transform_dag.py'][0m
[[34m2024-06-25T11:22:16.517+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'transform_dag', 'transform_task', 'manual__2024-06-25T11:22:15.677668+00:00', '--local', '--subdir', 'DAGS_FOLDER/transform_dag.py'][0m
[[34m2024-06-25T11:22:17.384+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/transform_dag.py[0m
[[34m2024-06-25T11:22:17.777+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T11:22:17.778+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T11:22:17.865+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T11:22:18.415+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: transform_dag.transform_task manual__2024-06-25T11:22:15.677668+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T11:22:19.122+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='transform_dag', task_id='transform_task', run_id='manual__2024-06-25T11:22:15.677668+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T11:22:19.130+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=transform_dag, task_id=transform_task, run_id=manual__2024-06-25T11:22:15.677668+00:00, map_index=-1, run_start_date=2024-06-25 11:22:18.490821+00:00, run_end_date=2024-06-25 11:22:18.711910+00:00, run_duration=0.221089, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-06-25 11:22:16.487988+00:00, queued_by_job_id=13, pid=7065[0m
[[34m2024-06-25T11:22:19.396+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun transform_dag @ 2024-06-25 11:22:15.677668+00:00: manual__2024-06-25T11:22:15.677668+00:00, state:running, queued_at: 2024-06-25 11:22:15.688369+00:00. externally triggered: True> successful[0m
[[34m2024-06-25T11:22:19.397+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=transform_dag, execution_date=2024-06-25 11:22:15.677668+00:00, run_id=manual__2024-06-25T11:22:15.677668+00:00, run_start_date=2024-06-25 11:22:16.400305+00:00, run_end_date=2024-06-25 11:22:19.397037+00:00, run_duration=2.996732, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-25 11:22:15.677668+00:00, data_interval_end=2024-06-25 11:22:15.677668+00:00, dag_hash=fa8368cdeb3e0e897cfd329e5d9f3900[0m
[[34m2024-06-25T11:26:39.983+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T11:31:40.146+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T11:36:40.306+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T11:41:40.514+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T11:46:40.675+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T11:51:40.833+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T11:56:40.992+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:01:41.252+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:06:41.413+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:11:41.673+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:16:41.808+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:17:29.122+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: load_dag.load_task manual__2024-06-25T12:17:28.439773+00:00 [scheduled]>[0m
[[34m2024-06-25T12:17:29.123+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG load_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:17:29.123+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: load_dag.load_task manual__2024-06-25T12:17:28.439773+00:00 [scheduled]>[0m
[[34m2024-06-25T12:17:29.124+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2024-06-25T12:17:28.439773+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-25T12:17:29.125+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2024-06-25T12:17:28.439773+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2024-06-25T12:17:29.154+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2024-06-25T12:17:28.439773+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2024-06-25T12:17:30.083+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/load_dag.py[0m
[[34m2024-06-25T12:17:30.175+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:17:30.175+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:17:30.267+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:17:31.197+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: load_dag.load_task manual__2024-06-25T12:17:28.439773+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:17:32.020+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2024-06-25T12:17:28.439773+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:17:32.023+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=load_dag, task_id=load_task, run_id=manual__2024-06-25T12:17:28.439773+00:00, map_index=-1, run_start_date=2024-06-25 12:17:31.291358+00:00, run_end_date=2024-06-25 12:17:31.597489+00:00, run_duration=0.306131, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-25 12:17:29.123922+00:00, queued_by_job_id=13, pid=36492[0m
[[34m2024-06-25T12:17:32.165+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun load_dag @ 2024-06-25 12:17:28.439773+00:00: manual__2024-06-25T12:17:28.439773+00:00, state:running, queued_at: 2024-06-25 12:17:28.449238+00:00. externally triggered: True> successful[0m
[[34m2024-06-25T12:17:32.165+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=load_dag, execution_date=2024-06-25 12:17:28.439773+00:00, run_id=manual__2024-06-25T12:17:28.439773+00:00, run_start_date=2024-06-25 12:17:29.045548+00:00, run_end_date=2024-06-25 12:17:32.165581+00:00, run_duration=3.120033, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-25 12:17:28.439773+00:00, data_interval_end=2024-06-25 12:17:28.439773+00:00, dag_hash=0bc02292f1233212c30a1a2607cb2769[0m
[[34m2024-06-25T12:21:41.967+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:26:42.177+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:27:16.353+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-06-25T12:27:15.977548+00:00 [scheduled]>[0m
[[34m2024-06-25T12:27:16.356+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:27:16.358+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.extract_task manual__2024-06-25T12:27:15.977548+00:00 [scheduled]>[0m
[[34m2024-06-25T12:27:16.366+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-06-25T12:27:15.977548+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-25T12:27:16.368+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-06-25T12:27:15.977548+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-06-25T12:27:16.434+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'extract_task', 'manual__2024-06-25T12:27:15.977548+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-06-25T12:27:17.271+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-06-25T12:27:17.690+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:27:17.691+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:27:17.777+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:27:18.324+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.extract_task manual__2024-06-25T12:27:15.977548+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:27:19.492+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='extract_task', run_id='manual__2024-06-25T12:27:15.977548+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:27:19.496+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=extract_task, run_id=manual__2024-06-25T12:27:15.977548+00:00, map_index=-1, run_start_date=2024-06-25 12:27:18.391029+00:00, run_end_date=2024-06-25 12:27:19.012423+00:00, run_duration=0.621394, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-06-25 12:27:16.359511+00:00, queued_by_job_id=13, pid=40859[0m
[[34m2024-06-25T12:27:19.670+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-06-25T12:27:15.977548+00:00 [scheduled]>[0m
[[34m2024-06-25T12:27:19.671+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:27:19.671+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.transform_task manual__2024-06-25T12:27:15.977548+00:00 [scheduled]>[0m
[[34m2024-06-25T12:27:19.674+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-06-25T12:27:15.977548+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-25T12:27:19.674+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-06-25T12:27:15.977548+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-06-25T12:27:19.710+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'transform_task', 'manual__2024-06-25T12:27:15.977548+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-06-25T12:27:20.646+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-06-25T12:27:21.052+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:27:21.053+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:27:21.150+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:27:21.734+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.transform_task manual__2024-06-25T12:27:15.977548+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:27:22.582+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='transform_task', run_id='manual__2024-06-25T12:27:15.977548+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:27:22.586+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=transform_task, run_id=manual__2024-06-25T12:27:15.977548+00:00, map_index=-1, run_start_date=2024-06-25 12:27:21.815649+00:00, run_end_date=2024-06-25 12:27:22.079792+00:00, run_duration=0.264143, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-25 12:27:19.671985+00:00, queued_by_job_id=13, pid=40900[0m
[[34m2024-06-25T12:27:22.788+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: basic_etl_dag.load_task manual__2024-06-25T12:27:15.977548+00:00 [scheduled]>[0m
[[34m2024-06-25T12:27:22.789+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG basic_etl_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:27:22.789+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: basic_etl_dag.load_task manual__2024-06-25T12:27:15.977548+00:00 [scheduled]>[0m
[[34m2024-06-25T12:27:22.792+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2024-06-25T12:27:15.977548+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-25T12:27:22.792+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2024-06-25T12:27:15.977548+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-06-25T12:27:22.820+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'basic_etl_dag', 'load_task', 'manual__2024-06-25T12:27:15.977548+00:00', '--local', '--subdir', 'DAGS_FOLDER/basic_etl_dag.py'][0m
[[34m2024-06-25T12:27:23.657+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/basic_etl_dag.py[0m
[[34m2024-06-25T12:27:24.043+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:27:24.043+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:27:24.130+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:27:24.664+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: basic_etl_dag.load_task manual__2024-06-25T12:27:15.977548+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:27:25.491+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='basic_etl_dag', task_id='load_task', run_id='manual__2024-06-25T12:27:15.977548+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:27:25.495+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=basic_etl_dag, task_id=load_task, run_id=manual__2024-06-25T12:27:15.977548+00:00, map_index=-1, run_start_date=2024-06-25 12:27:24.728900+00:00, run_end_date=2024-06-25 12:27:25.026299+00:00, run_duration=0.297399, state=success, executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-25 12:27:22.790345+00:00, queued_by_job_id=13, pid=40910[0m
[[34m2024-06-25T12:27:25.629+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun basic_etl_dag @ 2024-06-25 12:27:15.977548+00:00: manual__2024-06-25T12:27:15.977548+00:00, state:running, queued_at: 2024-06-25 12:27:16.028032+00:00. externally triggered: True> successful[0m
[[34m2024-06-25T12:27:25.629+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=basic_etl_dag, execution_date=2024-06-25 12:27:15.977548+00:00, run_id=manual__2024-06-25T12:27:15.977548+00:00, run_start_date=2024-06-25 12:27:16.216604+00:00, run_end_date=2024-06-25 12:27:25.629452+00:00, run_duration=9.412848, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-25 12:27:15.977548+00:00, data_interval_end=2024-06-25 12:27:15.977548+00:00, dag_hash=bae6be2e9d9b5d6377abab1b20839366[0m
[[34m2024-06-25T12:31:42.313+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:36:42.450+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:37:37.404+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: challenge_dag.extract_task manual__2024-06-25T12:37:36.969244+00:00 [scheduled]>[0m
[[34m2024-06-25T12:37:37.404+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG challenge_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:37:37.404+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: challenge_dag.extract_task manual__2024-06-25T12:37:36.969244+00:00 [scheduled]>[0m
[[34m2024-06-25T12:37:37.407+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='challenge_dag', task_id='extract_task', run_id='manual__2024-06-25T12:37:36.969244+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-25T12:37:37.408+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'challenge_dag', 'extract_task', 'manual__2024-06-25T12:37:36.969244+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:37:37.436+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'challenge_dag', 'extract_task', 'manual__2024-06-25T12:37:36.969244+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:37:38.210+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challange_dag.py[0m
[[34m2024-06-25T12:37:38.603+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:37:38.603+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:37:38.715+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:37:39.263+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: challenge_dag.extract_task manual__2024-06-25T12:37:36.969244+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:37:40.068+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='challenge_dag', task_id='extract_task', run_id='manual__2024-06-25T12:37:36.969244+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:37:40.075+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=challenge_dag, task_id=extract_task, run_id=manual__2024-06-25T12:37:36.969244+00:00, map_index=-1, run_start_date=2024-06-25 12:37:39.327019+00:00, run_end_date=2024-06-25 12:37:39.616525+00:00, run_duration=0.289506, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-06-25 12:37:37.405516+00:00, queued_by_job_id=13, pid=45443[0m
[[34m2024-06-25T12:37:41.059+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun challenge_dag @ 2024-06-25 12:37:36.969244+00:00: manual__2024-06-25T12:37:36.969244+00:00, state:running, queued_at: 2024-06-25 12:37:36.982135+00:00. externally triggered: True> failed[0m
[[34m2024-06-25T12:37:41.060+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=challenge_dag, execution_date=2024-06-25 12:37:36.969244+00:00, run_id=manual__2024-06-25T12:37:36.969244+00:00, run_start_date=2024-06-25 12:37:37.338811+00:00, run_end_date=2024-06-25 12:37:41.060019+00:00, run_duration=3.721208, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-06-25 12:37:36.969244+00:00, data_interval_end=2024-06-25 12:37:36.969244+00:00, dag_hash=87b7b49334599998e349171c9a7e6e52[0m
[[34m2024-06-25T12:41:00.889+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: challenge_dag.extract_task manual__2024-06-25T12:40:59.280607+00:00 [scheduled]>[0m
[[34m2024-06-25T12:41:00.889+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG challenge_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:41:00.889+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: challenge_dag.extract_task manual__2024-06-25T12:40:59.280607+00:00 [scheduled]>[0m
[[34m2024-06-25T12:41:00.892+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='challenge_dag', task_id='extract_task', run_id='manual__2024-06-25T12:40:59.280607+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-06-25T12:41:00.893+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'challenge_dag', 'extract_task', 'manual__2024-06-25T12:40:59.280607+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:41:00.922+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'challenge_dag', 'extract_task', 'manual__2024-06-25T12:40:59.280607+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:41:01.744+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challange_dag.py[0m
[[34m2024-06-25T12:41:02.147+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:41:02.148+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:41:02.235+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:41:02.912+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: challenge_dag.extract_task manual__2024-06-25T12:40:59.280607+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:41:03.821+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='challenge_dag', task_id='extract_task', run_id='manual__2024-06-25T12:40:59.280607+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:41:03.825+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=challenge_dag, task_id=extract_task, run_id=manual__2024-06-25T12:40:59.280607+00:00, map_index=-1, run_start_date=2024-06-25 12:41:02.981913+00:00, run_end_date=2024-06-25 12:41:03.333411+00:00, run_duration=0.351498, state=success, executor_state=success, try_number=1, max_tries=0, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2024-06-25 12:41:00.890455+00:00, queued_by_job_id=13, pid=46939[0m
[[34m2024-06-25T12:41:04.022+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: challenge_dag.transform_task manual__2024-06-25T12:40:59.280607+00:00 [scheduled]>[0m
[[34m2024-06-25T12:41:04.023+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG challenge_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:41:04.023+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: challenge_dag.transform_task manual__2024-06-25T12:40:59.280607+00:00 [scheduled]>[0m
[[34m2024-06-25T12:41:04.027+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='challenge_dag', task_id='transform_task', run_id='manual__2024-06-25T12:40:59.280607+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-25T12:41:04.027+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'challenge_dag', 'transform_task', 'manual__2024-06-25T12:40:59.280607+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:41:04.055+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'challenge_dag', 'transform_task', 'manual__2024-06-25T12:40:59.280607+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:41:04.945+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challange_dag.py[0m
[[34m2024-06-25T12:41:05.339+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:41:05.339+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:41:05.438+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:41:06.083+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: challenge_dag.transform_task manual__2024-06-25T12:40:59.280607+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:41:06.994+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='challenge_dag', task_id='transform_task', run_id='manual__2024-06-25T12:40:59.280607+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:41:06.997+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=challenge_dag, task_id=transform_task, run_id=manual__2024-06-25T12:40:59.280607+00:00, map_index=-1, run_start_date=2024-06-25 12:41:06.178935+00:00, run_end_date=2024-06-25 12:41:06.450031+00:00, run_duration=0.271096, state=success, executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-06-25 12:41:04.024375+00:00, queued_by_job_id=13, pid=46973[0m
[[34m2024-06-25T12:41:07.171+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: challenge_dag.load_task manual__2024-06-25T12:40:59.280607+00:00 [scheduled]>[0m
[[34m2024-06-25T12:41:07.171+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG challenge_dag has 0/16 running and queued tasks[0m
[[34m2024-06-25T12:41:07.171+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: challenge_dag.load_task manual__2024-06-25T12:40:59.280607+00:00 [scheduled]>[0m
[[34m2024-06-25T12:41:07.173+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='challenge_dag', task_id='load_task', run_id='manual__2024-06-25T12:40:59.280607+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-25T12:41:07.173+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'challenge_dag', 'load_task', 'manual__2024-06-25T12:40:59.280607+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:41:07.212+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'challenge_dag', 'load_task', 'manual__2024-06-25T12:40:59.280607+00:00', '--local', '--subdir', 'DAGS_FOLDER/challange_dag.py'][0m
[[34m2024-06-25T12:41:08.133+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challange_dag.py[0m
[[34m2024-06-25T12:41:08.548+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-25T12:41:08.549+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:41:08.640+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-25T12:41:09.192+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: challenge_dag.load_task manual__2024-06-25T12:40:59.280607+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-25T12:41:10.065+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='challenge_dag', task_id='load_task', run_id='manual__2024-06-25T12:40:59.280607+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-25T12:41:10.068+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=challenge_dag, task_id=load_task, run_id=manual__2024-06-25T12:40:59.280607+00:00, map_index=-1, run_start_date=2024-06-25 12:41:09.294266+00:00, run_end_date=2024-06-25 12:41:09.609658+00:00, run_duration=0.315392, state=success, executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-25 12:41:07.172329+00:00, queued_by_job_id=13, pid=46989[0m
[[34m2024-06-25T12:41:10.247+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun challenge_dag @ 2024-06-25 12:40:59.280607+00:00: manual__2024-06-25T12:40:59.280607+00:00, state:running, queued_at: 2024-06-25 12:40:59.287190+00:00. externally triggered: True> successful[0m
[[34m2024-06-25T12:41:10.247+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=challenge_dag, execution_date=2024-06-25 12:40:59.280607+00:00, run_id=manual__2024-06-25T12:40:59.280607+00:00, run_start_date=2024-06-25 12:41:00.821029+00:00, run_end_date=2024-06-25 12:41:10.247511+00:00, run_duration=9.426482, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-25 12:40:59.280607+00:00, data_interval_end=2024-06-25 12:40:59.280607+00:00, dag_hash=87b7b49334599998e349171c9a7e6e52[0m
[[34m2024-06-25T12:41:42.687+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:46:42.823+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:51:42.957+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-25T12:56:43.099+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
