[[34m2024-06-24T17:07:39.955+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2024-06-24T17:07:39.956+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-06-24T17:07:39.961+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 9304[0m
[[34m2024-06-24T17:07:39.963+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-24T17:07:39.966+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-06-24T17:07:39.992+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-06-24T17:07:40.129+0000[0m] {[34mscheduler_job_runner.py:[0m1576} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-06-24T17:10:16.512+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: one_task_dag.one_task manual__2024-06-24T17:10:14.598994+00:00 [scheduled]>[0m
[[34m2024-06-24T17:10:16.512+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG one_task_dag has 0/16 running and queued tasks[0m
[[34m2024-06-24T17:10:16.512+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: one_task_dag.one_task manual__2024-06-24T17:10:14.598994+00:00 [scheduled]>[0m
[[34m2024-06-24T17:10:16.515+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-06-24T17:10:14.598994+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-24T17:10:16.515+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-06-24T17:10:14.598994+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-06-24T17:10:16.560+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-06-24T17:10:14.598994+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-06-24T17:10:17.451+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/one_task_dag.py[0m
[[34m2024-06-24T17:10:17.543+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-24T17:10:17.543+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:10:17.631+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:10:18.449+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: one_task_dag.one_task manual__2024-06-24T17:10:14.598994+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-24T17:10:19.164+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-06-24T17:10:14.598994+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-24T17:10:19.171+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=one_task_dag, task_id=one_task, run_id=manual__2024-06-24T17:10:14.598994+00:00, map_index=-1, run_start_date=2024-06-24 17:10:18.511496+00:00, run_end_date=2024-06-24 17:10:18.758204+00:00, run_duration=0.246708, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-24 17:10:16.513409+00:00, queued_by_job_id=3, pid=10433[0m
[[34m2024-06-24T17:10:19.334+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun one_task_dag @ 2024-06-24 17:10:14.598994+00:00: manual__2024-06-24T17:10:14.598994+00:00, state:running, queued_at: 2024-06-24 17:10:14.609711+00:00. externally triggered: True> successful[0m
[[34m2024-06-24T17:10:19.335+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=one_task_dag, execution_date=2024-06-24 17:10:14.598994+00:00, run_id=manual__2024-06-24T17:10:14.598994+00:00, run_start_date=2024-06-24 17:10:16.394439+00:00, run_end_date=2024-06-24 17:10:19.335505+00:00, run_duration=2.941066, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-24 17:10:14.598994+00:00, data_interval_end=2024-06-24 17:10:14.598994+00:00, dag_hash=9496c75db0b7cde853d0ac97d4b69f06[0m
[[34m2024-06-24T17:11:53.033+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: one_task_dag.one_task manual__2024-06-24T17:11:51.350638+00:00 [scheduled]>[0m
[[34m2024-06-24T17:11:53.033+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG one_task_dag has 0/16 running and queued tasks[0m
[[34m2024-06-24T17:11:53.033+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: one_task_dag.one_task manual__2024-06-24T17:11:51.350638+00:00 [scheduled]>[0m
[[34m2024-06-24T17:11:53.035+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-06-24T17:11:51.350638+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-24T17:11:53.035+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-06-24T17:11:51.350638+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-06-24T17:11:53.061+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-06-24T17:11:51.350638+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-06-24T17:11:53.977+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/one_task_dag.py[0m
[[34m2024-06-24T17:11:54.070+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-24T17:11:54.070+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:11:54.190+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:11:55.001+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: one_task_dag.one_task manual__2024-06-24T17:11:51.350638+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-24T17:11:55.778+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-06-24T17:11:51.350638+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-24T17:11:55.782+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=one_task_dag, task_id=one_task, run_id=manual__2024-06-24T17:11:51.350638+00:00, map_index=-1, run_start_date=2024-06-24 17:11:55.071330+00:00, run_end_date=2024-06-24 17:11:55.354119+00:00, run_duration=0.282789, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-24 17:11:53.034338+00:00, queued_by_job_id=3, pid=11143[0m
[[34m2024-06-24T17:11:55.948+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun one_task_dag @ 2024-06-24 17:11:51.350638+00:00: manual__2024-06-24T17:11:51.350638+00:00, state:running, queued_at: 2024-06-24 17:11:51.364912+00:00. externally triggered: True> successful[0m
[[34m2024-06-24T17:11:55.948+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=one_task_dag, execution_date=2024-06-24 17:11:51.350638+00:00, run_id=manual__2024-06-24T17:11:51.350638+00:00, run_start_date=2024-06-24 17:11:52.956722+00:00, run_end_date=2024-06-24 17:11:55.948506+00:00, run_duration=2.991784, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-24 17:11:51.350638+00:00, data_interval_end=2024-06-24 17:11:51.350638+00:00, dag_hash=4eca649a32df60dae6540f629363d638[0m
[[34m2024-06-24T17:12:40.324+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-24T17:17:40.700+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-24T17:18:03.983+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-06-24T17:18:02.052732+00:00 [scheduled]>[0m
[[34m2024-06-24T17:18:03.983+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-06-24T17:18:03.983+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-06-24T17:18:02.052732+00:00 [scheduled]>[0m
[[34m2024-06-24T17:18:03.985+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-06-24T17:18:02.052732+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-06-24T17:18:03.985+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-06-24T17:18:02.052732+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-06-24T17:18:04.014+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-06-24T17:18:02.052732+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-06-24T17:18:04.914+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-06-24T17:18:05.005+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-24T17:18:05.005+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:18:05.093+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:18:05.940+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_0 manual__2024-06-24T17:18:02.052732+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-24T17:18:06.738+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-06-24T17:18:02.052732+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-24T17:18:06.741+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_0, run_id=manual__2024-06-24T17:18:02.052732+00:00, map_index=-1, run_start_date=2024-06-24 17:18:06.006242+00:00, run_end_date=2024-06-24 17:18:06.243309+00:00, run_duration=0.237067, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-06-24 17:18:03.984282+00:00, queued_by_job_id=3, pid=13968[0m
[[34m2024-06-24T17:18:06.946+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-06-24T17:18:02.052732+00:00 [scheduled]>[0m
[[34m2024-06-24T17:18:06.947+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-06-24T17:18:06.947+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-06-24T17:18:02.052732+00:00 [scheduled]>[0m
[[34m2024-06-24T17:18:06.948+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-06-24T17:18:02.052732+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-06-24T17:18:06.949+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-06-24T17:18:02.052732+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-06-24T17:18:06.980+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-06-24T17:18:02.052732+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-06-24T17:18:07.850+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-06-24T17:18:07.941+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/usr/local/python/3.10.13/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-06-24T17:18:07.942+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:18:08.029+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-06-24T17:18:08.845+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_1 manual__2024-06-24T17:18:02.052732+00:00 [queued]> on host codespaces-e400ad[0m
[[34m2024-06-24T17:18:14.591+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-06-24T17:18:02.052732+00:00', try_number=1, map_index=-1)[0m
[[34m2024-06-24T17:18:14.594+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_1, run_id=manual__2024-06-24T17:18:02.052732+00:00, map_index=-1, run_start_date=2024-06-24 17:18:08.924980+00:00, run_end_date=2024-06-24 17:18:14.170469+00:00, run_duration=5.245489, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-06-24 17:18:06.947798+00:00, queued_by_job_id=3, pid=13985[0m
[[34m2024-06-24T17:18:14.756+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun two_task_dag @ 2024-06-24 17:18:02.052732+00:00: manual__2024-06-24T17:18:02.052732+00:00, state:running, queued_at: 2024-06-24 17:18:02.061811+00:00. externally triggered: True> successful[0m
[[34m2024-06-24T17:18:14.756+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=two_task_dag, execution_date=2024-06-24 17:18:02.052732+00:00, run_id=manual__2024-06-24T17:18:02.052732+00:00, run_start_date=2024-06-24 17:18:03.917422+00:00, run_end_date=2024-06-24 17:18:14.756597+00:00, run_duration=10.839175, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-06-24 17:18:02.052732+00:00, data_interval_end=2024-06-24 17:18:02.052732+00:00, dag_hash=fbb5556d38ec0bba94cee596c221684f[0m
[[34m2024-06-24T17:22:40.874+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-06-24T17:27:41.038+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
